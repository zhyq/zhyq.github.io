<h3 id="关于word2vec我有话要说">关于word2vec我有话要说</h3>
<h4 id="写在前面的话">写在前面的话：</h4>
<p>总结一下自己使用word2vec的一些经验，因为自己在做的时候，很难在网上搜到word2vec的经验介绍， 所以总结一下，希望对读者有用。 这里不j介绍word2vec的原理，因为原理介绍的资料网上很多 <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">作者论文</a> <a href="https://groups.google.com/forum/#!forum/word2vec-toolkit">word2vec论坛</a> 作者论文讲的比较简单，推荐一个比较全面的<a href="http://www.cnblogs.com/peghoty/p/3857839.html">word2vec原理分析</a> 最后，由于本人知识有限，错误之处，还望指正。 知乎链接：https://zhuanlan.zhihu.com/p/29364112</p>
<h4 id="word2vec-是word-embedding-最好的工具吗">1 word2vec 是word embedding 最好的工具吗？</h4>
<p>word2vec并非是效果最好的word embedding 工具。最容易看出的就是word2vec没有考虑语序，这里会有训练效果损失。 由于 word2vec 训练速度快 ，易用，google出品 等，使得word2vec使用的人多。 训练快是因为 word2vec只有输入层和输出层，砍去了神经网络中，隐藏层的耗时计算（所以word2vec并不算是一个深度学习算法）。另外，阅读word2vec的google的源码，会发现里面有一些提速的trick。 如 sigmod函数，采用一次计算，以后查表，减去了大量的重复计算。如词典hash存储，层次softmax等。 易用是因为word2vec公布了word2vec的代码。在tensorflow,gensim,spark mllib包中都有集成，使用方便。</p>
<h4 id="word2vec-训练结果的差异主要来自什么因素">2 word2vec 训练结果的差异主要来自什么因素？</h4>
<ul>
<li>2.1 语料影响最大<br />
语料的场景，比如微博的语料和新闻语料训练的结果差别很大。因为微博属于个人发帖，比较随意。而新闻比较官方正式，另外新闻句式相对复杂。经过训练对比：微博这种短文，训练的相似词更多是同级别的相关词。比如 深圳 相关的是 广州 。而用新闻语料，训练得到 深圳 相关的词 更多是与 深圳 有关联的词，比如 深圳大学。 实际发现在微博，违法色情的词训练的比较好，因为黑产用这种聚到来推广。而在评论，骂人的词训练的比较好，在新闻，则是常见的正规的词训练的比较好。 新闻和评论 训练出垃圾 相关的词如下</li>
</ul>
<p><img src="http://zhangyunquan.cn/blog/pic/laji_news.PNG" alt="image" /> <img src="http://zhangyunquan.cn/blog/pic/laji_co.PNG" alt="image" /></p>
<p>为什么会出现这种情况呢？</p>
<p>因为 word2vec 的原理就是 一个词预测 前后词 或者 前后词 预测 当前词，使得概率最大化。 这就导致如下两个结果：</p>
<ul>
<li><p>2.1.1 相似的句子，相同部位的词 会相似。 比如 句子1 w1 w2 w3 w4 <code>X</code> w5 w6 w7. 句子2 w1 w2 w3 w5 <code>Y</code> w5 w6 w7. 因为 <code>X</code> 的向量 受 w1 w2 w3 w4 w5 w6 w7 向量影响决定， <code>Y</code> 也是受这几个词影响决定。 所以 <code>X</code> <code>Y</code> 是相似的。</p></li>
<li><p>2.1.2 挨着近的词，也是相似的。</p></li>
</ul>
<p>比如 句子 w1 w2 w3 w4 <code>X</code> <code>Y</code> w5 w6 w7. 这样 <code>X</code> <code>Y</code> 都是受到 来自 w1 w2 w3 w4 w5 w6 w7 向量影响决定。 所以<code>X</code> <code>Y</code>是相似的。 所以，微博和新闻的句子的整体分布是不一样的。 这里影响 结论 2.1.1. 其次，新闻长文多，句式复杂，微博短文多，这里影响结论2.1.2.</p>
<ul>
<li><p>2.2 算法参数的影响。 算法参数对总体效果影响不大。相对来说，比较重要的参数有以下</p></li>
<li>2.2.1 降采样。 降采样越低，对高频词越不利，对低频词有利。可以这么理解，本来高频词 词被迭代50次，低频词迭代10次，如果采样频率降低一半，高频词失去了25次迭代，而低频词只失去了5次。一般设置成le-5</li>
<li>2.2.2 语言模型 skip-gram 和cbow,之前有对比，切词效果偏重各不相同。从效果来看，感觉cbow对词频低的词更有利 这是因为 cbow是基于周围词来预测某个词，虽然这个词词频低，但是他是基于 周围词训练的基础上，通过算法来得到这个词的向量。通过周围词的影响，周围词训练的充分，这个词就会收益。</li>
<li>2.2.3 窗口大小 窗口大小影响 词 和前后多少个词的关系，和语料中语句长度有关，建议可以统计一下语料中，句子长度的分布，再来设置window大小。一般设置成8。</li>
<li>2.2.4 min-count 最小词频训练阀值 这个根据训练语料大小设置，只有词频超过这个阀值的词才能被训练。 根据经验，如果切词效果不好，会切错一些词，比如 “在深圳”，毕竟切错的是少数情况，使得这种错词词频不高，可以通过设置相对大一点的 min-count 过滤掉切错的词。</li>
<li>2.2. 5 向量维度<br />
如果词量大，训练得到的词向量还要做语义层面的叠加，比如 句子 的向量表示 用 词的向量叠加，为了有区分度，语义空间应该要设置大一些，所以维度要偏大。一般 情况下200维够用。</li>
<li><p>2.2.6 其他参数 比如学习率 可以根据需要调。</p></li>
</ul>
<h4 id="word2vec-影响速度的因素有哪些">3 word2vec 影响速度的因素有哪些？</h4>
<ul>
<li>3.1 语言模型：cbow 比skip-gram 更快 为什么 cbow更快，很重要的一个原因，cbow是基于周围词来预测这个单词本身。而skip-gram是基于本身词去预测周围词。 那么，cbow只要 把窗口内的其他词相加一次作为输入来预测 一个单词。不管窗口多大，只需要一次运算。 而skip-gram直接受窗口影响，窗口越大，需要预测的周围词越多。在训练中，通过调整窗口大小明显感觉到训练速度受到很大影响。</li>
<li>3.2 迭代次数 影响训练次数，语料不够的情况下，可以调大迭代次数。spark 版本有bug，迭代次数超过1，训练得到的词向量维度值超大。</li>
<li>3.3 线程数 单机版（google word2vec)可以通过设置多线程跑,集群版（spark mllib）可以设置多个 partitions.但是从经验来看，在集群上设置partitions 过多，会影响训练的效果。</li>
<li>3.4 其他参数 采样频率 影响词的训练频率 min-count 最小词频 影响 训练词的数量 Window大小 影响 skip-gram 的 预测次数。 向量维度 维度决定了训练过程中计算的维度</li>
</ul>
<h4 id="怎样评估word2vec训练的好坏">4 怎样评估word2vec训练的好坏？</h4>
<ul>
<li>4.1 词聚类 可以采用 kmeans 聚类，看聚类簇的分布</li>
<li>4.2词cos 相关性 查找cos相近的词</li>
<li>4.3 Analogy对比 a:b 与 c:d的cos距离 (man-king woman-queen )</li>
<li>4.4 使用tnse，pca等降维可视化展示 词的分布，推荐用google的tensorboard[https://www.tensorflow.org/guide/summaries_and_tensorboard] ，可以多视角查看,如果不想搭建服务，直接访问这里另外可以用python的matplotlib。</li>
<li>4.5 Categorization 分类 看词在每个分类中的概率 词 动物 食物 汽车 电子<br />
橘子 0.11 0.68 0.12 0.11 鸟 0.66 0.11 0.13 0.11 雅阁 0.14 0.23 0.67 0.11 苹果 0.11 0.65 0.11 0.65 前三条来自官网的评测方法 网上也有相关的word embedding 的评估方法，可以参考这里</li>
<li>4.6 根据 同义词 相关词 反义词 来评定(同义词 上下位词 相关词 反义词) 同义词 分值为 5：（农业银行与农行） 相关词 分值为 3：（农业银行与建设银行） 反义词 分值为1：（微笑与哭泣） 通过对比每个目标词的相似词列表和人工标注的词进行评估，得出分数</li>
</ul>
<h4 id="资源">资源</h4>
<ul>
<li><p><a href="https://github.com/zhyq/word2vec-google">google word2vec</a></p></li>
<li><p><a href="https://github.com/zhyq/word2vec-spark">spark mllib word2vec 转换成word vector形式(与google一致)</a></p></li>
</ul>
<h4 id="word2vec效果展示">word2vec效果展示</h4>
<p><img src="http://zhangyunquan.cn/blog/pic/word2vec_result.png" alt="image" /> </p>
